#include <metal_stdlib>
using namespace metal;

// Constants for neural network parameters
constant float DECAY_RATE = 0.8f;
constant float INPUT_WEIGHT = 0.1f;
constant float CONNECTION_WEIGHT = 0.2f;
constant float ACTIVATION_SCALE = 1.5f;
constant float ACTIVATION_BIAS = 0.1f;
constant float MIN_ACTIVATION = -1.0f;
constant float MAX_ACTIVATION = 1.0f;
constant float LEARNING_RATE = 0.01f;
constant float WEIGHT_DECAY = 0.1f;
constant float MIN_WEIGHT = -1.0f;
constant float MAX_WEIGHT = 1.0f;
constant uint MAX_NEURONS = 1024;      // Adjust as needed no idea why i made it so big
constant uint MAX_CONNECTIONS = 16;    // Adjust as needed
constant uint ACTIVATION_TANH = 0;
constant uint ACTIVATION_RELU = 1;
constant uint ACTIVATION_SIGMOID =  2;
constant uint ACTIVATION_LEAKY_RELU =  3;
constant uint ACTIVATION_SWISH = 4;


struct MemoryEntry {
    float vector[MAX_NEURONS * 2]; // Stores neuron states and outputs
    float importance;              // Importance of the memory
    uint timestamp;                // Timestamp of memory creation
};

struct Neuron {
    float state;
    float output;
    uint num_connections;
    uint layer_id;
};

// Fast approximation of tanh for better performance
static inline float fast_tanh(float x) {
    float x2 = x * x;
    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));
    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));
    return clamp(a / b, MIN_ACTIVATION, MAX_ACTIVATION);
}

// ReLU activation function
static inline float relu(float x) {
    return metal::max(0.0f, x);
}

// Sigmoid activation function
static inline float sigmoid(float x) {
    return 1.0f / (1.0f + metal::exp(-x));
}

// Leaky ReLU activation function
static inline float leaky_relu(float x, float alpha = 0.01f) {
    return x > 0.0f ? x : alpha * x;
}

// Swish activation function (x * sigmoid(x))
static inline float swish(float x) {
    return x * sigmoid(x);
}

// Activation function with configurable response curve and type
static inline float activation_function(float x, float scale, float bias, uint activation_type) {
    // Apply scale and bias
    float scaled = x * scale + bias;
    
    // Select activation function based on type
    float base_activation;
    switch (activation_type) {
        case ACTIVATION_RELU:
            base_activation = relu(scaled);
            break;
        case ACTIVATION_SIGMOID:
            base_activation = sigmoid(scaled);
            break;
        case ACTIVATION_LEAKY_RELU:
            base_activation = leaky_relu(scaled);
            break;
        case ACTIVATION_SWISH:
            base_activation = swish(scaled);
            break;
        case ACTIVATION_TANH:
        default:
            base_activation = fast_tanh(scaled);
            break;
    }
    
    // Add nonlinearity for more dynamic response
    float sign_val = metal::sign(base_activation);
    float abs_val = metal::abs(base_activation);
    
    // Apply additional nonlinearity for tanh and sigmoid only
    if (activation_type == ACTIVATION_TANH || activation_type == ACTIVATION_SIGMOID) {
        return sign_val * metal::pow(abs_val, 1.1f);
    } else {
        return base_activation;
    }
}

kernel void update_neurons(device Neuron* neurons [[buffer(0)]],
                         device const float* weights [[buffer(1)]],
                         device const uint* connections [[buffer(2)]],
                         device const uint& max_neurons [[buffer(3)]],
                         device const uint& max_connections [[buffer(4)]],
                         device const float* input_tensor [[buffer(5)]],
                         device const uint& input_size [[buffer(6)]],
                         device const float* recurrent_weights [[buffer(7)]],
                         device const uint& activation_type [[buffer(8)]],
                         uint id [[thread_position_in_grid]]) {
    // Early exit for out of bounds threads
    if (id >= max_neurons) return;
    
    // Load neuron data into thread-local storage
    float current_state = neurons[id].state;
    float current_output = neurons[id].output;
    uint num_conn = neurons[id].num_connections;
    uint layer = neurons[id].layer_id;
    
    // Calculate weighted sum of inputs from connected neurons
    float weighted_sum = 0.0f;
    
    // Process connections
    for (uint i = 0; i < num_conn; i++) {
        uint conn_idx = id * max_connections + i;
        uint target = connections[conn_idx];
        
        // Add weight scaling based on layer depth
        float depth_scale = 1.0f / (1.0f + layer);
        float connection_strength = weights[conn_idx] * depth_scale;
        
        // Combine state and output influences
        weighted_sum += neurons[target].state * connection_strength * 0.6f +
                       neurons[target].output * connection_strength * 0.4f;
    }
    
    // Calculate input influence with temporal dynamics
    float input_influence = input_tensor[id % input_size];
    float temporal_factor = 1.0f / (1.0f + id % 4); // Creates wave-like temporal patterns
    
    // Update state with multiple influences
    float new_state = current_state * DECAY_RATE +
                     weighted_sum * CONNECTION_WEIGHT +
                     input_influence * INPUT_WEIGHT * temporal_factor;
    
    // Add recurrent connection influence
    float recurrent_influence = current_output * recurrent_weights[id];
    new_state += recurrent_influence * 0.15f;
    
    // Apply activation function with dynamic scaling
    float dynamic_scale = ACTIVATION_SCALE * (1.0f + 0.1f * metal::sin(input_influence * M_PI_F));
    float new_output = activation_function(new_state, dynamic_scale, ACTIVATION_BIAS, activation_type);
    
    // Add slight randomization for variability
    float random_val = metal::fract(metal::sin(dot(float2(float(id), new_state), 
                                                  float2(12.9898f, 78.233f))) * 43758.5453f);
    new_output += random_val * 0.01f;
    
    // Ensure outputs stay within valid range
    new_output = metal::clamp(new_output, MIN_ACTIVATION, MAX_ACTIVATION);
    
    // Write back results
    neurons[id].state = new_state;
    neurons[id].output = new_output;
}

// Weight update kernel
kernel void update_weights(device float* weights [[buffer(0)]],
                         device const Neuron* neurons [[buffer(1)]],
                         device const uint* connections [[buffer(2)]],
                         device const float& learning_rate [[buffer(3)]],
                         device const uint& max_neurons [[buffer(4)]],
                         device const uint& max_connections [[buffer(5)]],
                         uint id [[thread_position_in_grid]]) {
    if (id >= max_neurons * max_connections) return;
    
    uint neuron_idx = id / max_connections;
    uint conn_idx = id % max_connections;
    
    if (conn_idx >= neurons[neuron_idx].num_connections) return;
    
    uint target_idx = connections[id];
    
    // Modified Hebbian learning with normalization
    float pre_activation = neurons[neuron_idx].state;
    float post_activation = neurons[target_idx].output;
    float current_weight = weights[id];
    
    // Calculate weight update
    float hebbian_term = pre_activation * post_activation;
    float normalization_term = current_weight * WEIGHT_DECAY;
    float delta_w = learning_rate * (hebbian_term - normalization_term);
    
    // Update weight with momentum
    float momentum = 0.9f;
    float new_weight = current_weight + delta_w;
    new_weight = momentum * current_weight + (1.0f - momentum) * new_weight;
    
    // Clip weights
    weights[id] = metal::clamp(new_weight, MIN_WEIGHT, MAX_WEIGHT);
}

kernel void process_neurons(device Neuron* neurons [[buffer(0)]],
                          device const float* weights [[buffer(1)]],
                          device const uint* connections [[buffer(2)]],
                          device const uint& max_neurons [[buffer(3)]],
                          device const uint& max_connections [[buffer(4)]],
                          device const float* input_tensor [[buffer(5)]],
                          device const uint& input_size [[buffer(6)]],
                          device const float* recurrent_weights [[buffer(7)]],
                          device const uint& activation_type [[buffer(8)]],
                          uint id [[thread_position_in_grid]]) {
    if (id >= max_neurons) return;
    
    float current_state = neurons[id].state;
    float current_output = neurons[id].output;
    uint num_conn = neurons[id].num_connections;
    uint layer = neurons[id].layer_id;
    
    // Calculate weighted sum
    float weighted_sum = 0.0f;
    for (uint i = 0; i < num_conn; i++) {
        uint conn_idx = id * max_connections + i;
        uint target = connections[conn_idx];
        
        float depth_scale = 1.0f / (1.0f + layer);
        float connection_strength = weights[conn_idx] * depth_scale;
        
        weighted_sum += neurons[target].state * connection_strength * 0.6f +
                       neurons[target].output * connection_strength * 0.4f;
    }
    
    // Input processing with temporal dynamics
    float input_influence = input_tensor[id % input_size];
    float temporal_factor = 1.0f / (1.0f + id % 4);
    
    // State update with multiple influences
    float new_state = current_state * DECAY_RATE +
                     weighted_sum * CONNECTION_WEIGHT +
                     input_influence * INPUT_WEIGHT * temporal_factor;
    
    // Add recurrent influence
    float recurrent_influence = current_output * recurrent_weights[id];
    new_state += recurrent_influence * 0.15f;
    
    // Dynamic activation
    float dynamic_scale = ACTIVATION_SCALE * (1.0f + 0.1f * metal::sin(input_influence * M_PI_F));
    float new_output = activation_function(new_state, dynamic_scale, ACTIVATION_BIAS, activation_type);
    
    // Add controlled randomization
    float random_val = metal::fract(metal::sin(dot(float2(float(id), new_state),
                                                  float2(12.9898f, 78.233f))) * 43758.5453f);
    new_output += random_val * 0.01f;
    
    // Clamp output
    new_output = metal::clamp(new_output, MIN_ACTIVATION, MAX_ACTIVATION);
    
    // Store results
    neurons[id].state = new_state;
    neurons[id].output = new_output;
}

kernel void backwardKernel(
    const device Neuron *neurons [[buffer(0)]],         
    device float *weights [[buffer(1)]],        
    const device uint2 *connections [[buffer(2)]],     
    const device uint *maxNeurons [[buffer(3)]],       
    const device uint *maxConnections [[buffer(4)]],    
    const device float *targetOutputs [[buffer(5)]],    
    device float *outputErrors [[buffer(6)]],           
    device float *m [[buffer(7)]],   // First moment buffer for Adam
    device float *v [[buffer(8)]],   // Second moment buffer for Adam
    const device float *beta1 [[buffer(9)]],   
    const device float *beta2 [[buffer(10)]],  
    const device float *epsilon [[buffer(11)]], 
    const device float *learningRate [[buffer(12)]],     
    const device uint *timeSteps [[buffer(13)]],  
    device uint *t [[buffer(14)]],  // Optimization step counter
    uint gid [[thread_position_in_grid]]) { 

    if (gid >= *maxNeurons) return;

    float totalError = 0.0;

    // Iterate over time steps in reverse (BPTT)
    for (int step = *timeSteps - 1; step >= 0; step--) {
        uint neuronIndex = gid * (*timeSteps) + step;
        float predictedOutput = neurons[neuronIndex].output;
        float targetOutput = targetOutputs[neuronIndex];

        // Compute error
        float error = predictedOutput - targetOutput;
        float activationGradient = predictedOutput * (1.0 - predictedOutput); // Sigmoid derivative
        totalError += error;

        // Store error for analysis
        outputErrors[neuronIndex] = error;

        // Compute gradients and update weights
        for (uint i = 0; i < *maxConnections; i++) {
            uint2 connection = connections[gid * (*maxConnections) + i];
            uint connectedNeuron = connection.x;
            uint weightIndex = connection.y;

            if (connectedNeuron >= *maxNeurons) continue;

            float inputGradient = neurons[connectedNeuron * (*timeSteps) + step].output * totalError * activationGradient;

            // Adam Optimizer updates
            float beta1_t = *beta1;
            float beta2_t = *beta2;
            float epsilon_t = *epsilon;
            uint timeStep = *t;

            // Compute first and second moment estimates
            m[weightIndex] = beta1_t * m[weightIndex] + (1.0 - beta1_t) * inputGradient;
            v[weightIndex] = beta2_t * v[weightIndex] + (1.0 - beta2_t) * (inputGradient * inputGradient);

            // Bias correction
            float mHat = m[weightIndex] / (1.0 - pow(beta1_t, timeStep));
            float vHat = v[weightIndex] / (1.0 - pow(beta2_t, timeStep));

            // Adam weight update
            weights[weightIndex] -= (*learningRate) * mHat / (sqrt(vHat) + epsilon_t);
        }
    }

    // Increment the optimization step counter
    atomic_fetch_add_explicit((device atomic_uint *)t, 1, memory_order_relaxed);
}


kernel void reverse_process(
    device Neuron *neurons [[buffer(0)]],
    device float *reverse_weights [[buffer(1)]],
    device uint *reverse_connections [[buffer(2)]],
    constant uint &max_neurons [[buffer(3)]],
    constant uint &max_connections [[buffer(4)]],
    uint id [[thread_position_in_grid]])
{
    if (id >= max_neurons) return;

    float sum = 0.0f;
    for (uint c = 0; c < max_connections; ++c) {
        uint conn_idx = id * max_connections + c;
        uint source_neuron = reverse_connections[conn_idx];

        // Accumulate contributions from reverse connections
        sum += neurons[source_neuron].output * reverse_weights[conn_idx];
    }

    // Update neuron state based on reverse pathway
    neurons[id].state += 0.1f * sum; // Adjust the scaling factor as needed
}

kernel void memory_replay(
    device Neuron *neurons [[buffer(0)]],
    device float *weights [[buffer(1)]],
    device uint *connections [[buffer(2)]],
    device MemoryEntry *memories [[buffer(3)]],
    constant uint &memory_capacity [[buffer(4)]],
    uint id [[thread_position_in_grid]])
{
    if (id >= memory_capacity) return;

    // Retrieve the memory entry
    MemoryEntry memory = memories[id];

    // Reinforce weights based on memory importance
    for (uint i = 0; i < MAX_NEURONS; ++i) {
        for (uint j = 0; j < MAX_CONNECTIONS; ++j) {
            uint conn_idx = i * MAX_CONNECTIONS + j;
            uint target_neuron = connections[conn_idx];

            // Update weights based on memory importance and neuron states
            float weight_delta = 0.01f * memory.importance * 
                                 neurons[i].output * 
                                 memory.vector[target_neuron];
            weights[conn_idx] += weight_delta;
        }
    }
}
